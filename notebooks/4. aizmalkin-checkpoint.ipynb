{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Let's consider the classification problem into the following two classes:\n",
    "- 1 for 'American_movie_actors'\n",
    "- 0 for 'American_stage_actors'\n",
    "\n",
    "Instead of a dictionary, you can use hashing.\n",
    "\n",
    "You are invited to check how the model behaves after using hashing and answer the following questions:\n",
    "1. **What roc_auc_score on the test sample is obtained when using a dictionary?**\n",
    "2. **What roc_auc_score on the test sample is obtained when switching from dictionary to hashing?**\n",
    "\n",
    "Details:\n",
    "1. Divide the samples into training and test by parity `id` articles: even for training, odd for test. Only for the training part, we count the gradients!\n",
    "2. To calculate roc_auc_score, you need to get predictions and true answers for examples from the test set. All pairs (prediction, answer) fit into memory, use it!\n",
    "3. Use `murmurhash3_32(x) % 2**20` as the hash function.\n",
    "4. Fix the random seed at the initial guess of the weights: `np.random.seed(0); weights = np.random.random(...)`\n",
    "5. Train 500 epochs in 0.3 increments. After each epoch, call `weights_broadcast.destroy()` to remove the broadcast variable so you don't run out of memory.\n",
    "6. This is what roc_auc_score looks like on the test sample from the number of epochs (the more roc_auc_score, the better):\n",
    "<img src=\"images/test_auc.png\" width=\"600px\"></img>\n",
    "\n",
    "Save the solution to the `result.json` file. \n",
    "File content example:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 0.123,\n",
    "    \"q2\": 0.456\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import murmurhash3_32 as mur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# y_true - real classes\n",
    "# y_score - class 1 probabilities\n",
    "# https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "roc_auc_score(y_true=[1, 1, 0, 0], y_score=[0.8, 0.7, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset preparation for a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-03-30 17:23:04,863 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! hadoop fs -copyFromLocal wiki /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April\\n\\nApril is the fourth month of the year...</td>\n",
       "      <td>April</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August\\n\\nAugust (Aug.) is the eighth month of...</td>\n",
       "      <td>August</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text   title  \\\n",
       "0  1  April\\n\\nApril is the fourth month of the year...   April   \n",
       "1  2  August\\n\\nAugust (Aug.) is the eighth month of...  August   \n",
       "\n",
       "                                         url  \n",
       "0  https://simple.wikipedia.org/wiki?curid=1  \n",
       "1  https://simple.wikipedia.org/wiki?curid=2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = se.read.json(\"hdfs:///wiki/wiki.jsonl\")\n",
    "wiki.registerTempTable(\"wiki\")\n",
    "wiki.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Months</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Months</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  page_id\n",
       "0   Months        1\n",
       "1   Months        2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = se.read.json(\"hdfs:///wiki/categories.jsonl\")\n",
    "categories.registerTempTable(\"categories\")\n",
    "categories.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5692</td>\n",
       "      <td>Natalie Portman\\n\\nNatalie Portman (born Neta-...</td>\n",
       "      <td>Natalie Portman</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=5692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5692</td>\n",
       "      <td>Natalie Portman\\n\\nNatalie Portman (born Neta-...</td>\n",
       "      <td>Natalie Portman</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=5692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5240</td>\n",
       "      <td>Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...</td>\n",
       "      <td>Elizabeth Taylor</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=5240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5240</td>\n",
       "      <td>Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...</td>\n",
       "      <td>Elizabeth Taylor</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=5240</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5230</td>\n",
       "      <td>Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...</td>\n",
       "      <td>Gwyneth Paltrow</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=5230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text             title  \\\n",
       "0  5692  Natalie Portman\\n\\nNatalie Portman (born Neta-...   Natalie Portman   \n",
       "1  5692  Natalie Portman\\n\\nNatalie Portman (born Neta-...   Natalie Portman   \n",
       "2  5240  Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...  Elizabeth Taylor   \n",
       "3  5240  Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...  Elizabeth Taylor   \n",
       "4  5230  Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...   Gwyneth Paltrow   \n",
       "\n",
       "                                            url  target  \n",
       "0  https://simple.wikipedia.org/wiki?curid=5692       0  \n",
       "1  https://simple.wikipedia.org/wiki?curid=5692       1  \n",
       "2  https://simple.wikipedia.org/wiki?curid=5240       0  \n",
       "3  https://simple.wikipedia.org/wiki?curid=5240       1  \n",
       "4  https://simple.wikipedia.org/wiki?curid=5230       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = se.sql(\"\"\"\n",
    "select\n",
    "    id,\n",
    "    text,\n",
    "    title,\n",
    "    url,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('American_movie_actors', 'American_stage_actors')\n",
    "\"\"\")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.write.mode('overwrite').json(\"/actors.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalie Portman\\n\\nNatalie Portman (born Neta-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natalie Portman\\n\\nNatalie Portman (born Neta-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Natalie Portman\\n\\nNatalie Portman (born Neta-...       0\n",
       "1  Natalie Portman\\n\\nNatalie Portman (born Neta-...       1\n",
       "2  Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...       0\n",
       "3  Elizabeth Taylor\\n\\nDame Elizabeth \"Liz\" Rosem...       1\n",
       "4  Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = se.sql(\"\"\"\n",
    "select\n",
    "    text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('American_movie_actors', 'American_stage_actors')\n",
    "and id % 2 = 0\n",
    "\"\"\")\n",
    "df_train.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Britney Spears\\n\\nBritney Jean Spears (born De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelina Jolie\\n\\nAngelina Jolie (; née Voight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brad Pitt\\n\\nWilliam Bradley \"Brad\" Pitt (born...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nicole Kidman\\n\\nNicole Mary Kidman, AC (born ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christina Ricci\\n\\nChristina Ricci (born Febru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Britney Spears\\n\\nBritney Jean Spears (born De...       1\n",
       "1  Angelina Jolie\\n\\nAngelina Jolie (; née Voight...       1\n",
       "2  Brad Pitt\\n\\nWilliam Bradley \"Brad\" Pitt (born...       1\n",
       "3  Nicole Kidman\\n\\nNicole Mary Kidman, AC (born ...       1\n",
       "4  Christina Ricci\\n\\nChristina Ricci (born Febru...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = se.sql(\"\"\"\n",
    "select\n",
    "    text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('American_movie_actors', 'American_stage_actors')\n",
    "and id % 2 <> 0\n",
    "\"\"\")\n",
    "df_test.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Create index of words regards with their population in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(f'[^{re.escape(string.printable)}]', ' ', text)  # replace unprintable characters with a space\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # and punctuation\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def mapper(line):\n",
    "    text = json.loads(line)['text']\n",
    "    words = tokenize(text)\n",
    "    return [(word, 1) for word in set(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==========================================>             (24 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 ms, sys: 8.11 ms, total: 46.6 ms\n",
      "Wall time: 5.77 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = (\n",
    "    sc.textFile(\"hdfs:///actors.jsonl\")\n",
    "    .flatMap(mapper)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23833"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american', 5519), ('in', 5381), ('an', 5315), ('and', 5152), ('the', 5119)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts = sorted(word_counts, key=lambda x: -x[1])[:10000]\n",
    "top_word_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicely', 3), ('hahn', 3), ('feeney', 3), ('jigsaw', 3), ('portlandia', 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indexes are needed for vectorization of texts\n",
    "word_to_index = {word: index for index, (word, count) in enumerate(top_word_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american', 0), ('in', 1), ('an', 2), ('and', 3), ('the', 4)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_index.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the train dataset \"df_train\" to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # first option: the word_to_index dictionary will be serialized using pickle along with the function\n",
    "# import numpy as np\n",
    "\n",
    "# def mapper(line):\n",
    "#     j = json.loads(line)\n",
    "#     text = j['text']\n",
    "#     words = tokenize(text)\n",
    "#     indices = []\n",
    "#     values = []\n",
    "#     for word, count in Counter(words).items():\n",
    "#         if word in word_to_index:\n",
    "#             index = word_to_index[word]\n",
    "#             indices.append(index)\n",
    "#             tf = count / float(len(words))\n",
    "#             values.append(tf)\n",
    "#     return np.array(indices), np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# (\n",
    "#     sc.textFile(\"hdfs:///actors.jsonl\")\n",
    "#     .map(mapper)\n",
    "#     .take(1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2814"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train.rdd.map(mapper)\n",
    "dataset.cache()  # cache dataset in RAM\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2135, 8313,    5,  372, 5017, 5072, 5031, 2916, 4059, 5269,   69,\n",
       "          166,  288,    8,    2, 4251,    0,   18,   21,    6,    1, 8015,\n",
       "         3445,  394,  217,  397,  407,  345,  443,    3,  482,  218, 1375,\n",
       "           19,  309,  554,   35,   40,   37,   29,  274,  194,  597,   22,\n",
       "          135,  100,  560, 1947,   95, 4265, 4844,  145,   49,    4,   83,\n",
       "          853,  105,  130,  106,    7,   27,   96,   17,  421, 4158,  126,\n",
       "          181,  202,  134, 2291,  489,  151,  376,  599,   15,  262, 6592,\n",
       "          846,   36,   25,   48,   10,  535,  179,  222]),\n",
       "  array([0.01709402, 0.03418803, 0.01709402, 0.00854701, 0.01709402,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.02564103, 0.01709402,\n",
       "         0.00854701, 0.00854701, 0.01709402, 0.05128205, 0.00854701,\n",
       "         0.05982906, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.02564103,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.01709402,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.02564103, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.02564103,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701]),\n",
       "  0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare auxulary functions and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1. + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(weights_broadcast, loss, examples):\n",
    "    # here we accumulate the contribution to the gradient\n",
    "    gradient = np.zeros(len(weights_broadcast.value))\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # make a prediction with the current weights\n",
    "        p = sigmoid(values.dot(weights_broadcast.value[indices]))\n",
    "\n",
    "        # add to gradient accumulator\n",
    "        gradient[indices] += values * (p - target)\n",
    "\n",
    "        # count losses\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        loss.add(-(target * np.log(p) + (1 - target) * np.log(1 - p)))\n",
    "    \n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2814"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of examples\n",
    "N = dataset.count()\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 499 loss: 0.4831161902131253\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# random weights\n",
    "np.random.seed(0)\n",
    "weights = np.random.random(len(word_to_index))\n",
    "\n",
    "# Gradient Descent Epoch\n",
    "for i in range(500):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # calculate the gradient\n",
    "    gradient = (\n",
    "        dataset\n",
    "        .coalesce(2)  # merge 200 cached partitions into 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # update the weights\n",
    "    weights -= 0.4 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    \n",
    "    # print(\"epoch:\", i, \"loss:\", loss.value / N)\n",
    "print(\"epoch:\", i, \"loss:\", loss.value / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.59426268, -0.81586375, 14.76037071, ...,  0.75842952,\n",
       "         0.87954432,  0.81357508]),\n",
       " 10000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, len(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5: Calculate \"roc_auc_score\" for test dataset \"df_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2785"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_test.rdd.map(mapper)\n",
    "dataset.cache()  # cache dataset in RAM\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rocAuc(row):\n",
    "    \n",
    "    indices, values, target = row\n",
    "    # make a prediction with the current weights\n",
    "    p = sigmoid(values.dot(weights[indices]))\n",
    "    \n",
    "    return p, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.891036899042322, 1), (0.8581572602570084, 1), (0.7583840538462286, 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocAuc_dataset = dataset.map(rocAuc)\n",
    "rocAuc_dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_score = rocAuc_dataset.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_true = rocAuc_dataset.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6857984028894291"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create index of words regards with their population in a text using hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(f'[^{re.escape(string.printable)}]', ' ', text)  # replace unprintable characters with a space\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # and punctuation\n",
    "    words = list(map(lambda x: mur(x) % 2**20, text.lower().split()))\n",
    "    return words\n",
    "\n",
    "def mapper(line):\n",
    "    text = json.loads(line)['text']\n",
    "    words = tokenize(text)\n",
    "    return [(word, 1) for word in set(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1047:==================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 13.6 ms, total: 37.7 ms\n",
      "Wall time: 5.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = (\n",
    "    sc.textFile(\"hdfs:///actors.jsonl\")\n",
    "    .flatMap(mapper)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23572"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(510525, 5519),\n",
       " (828689, 5381),\n",
       " (715111, 5315),\n",
       " (868051, 5152),\n",
       " (761698, 5119)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts = sorted(word_counts, key=lambda x: -x[1])[:10000]\n",
    "top_word_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(174814, 3), (257118, 3), (820126, 3), (516702, 3), (216318, 3)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indexes are needed for vectorization of texts\n",
    "word_to_index = {word: index for index, (word, count) in enumerate(top_word_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(510525, 0), (828689, 1), (715111, 2), (868051, 3), (761698, 4)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_index.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare the train dataset \"df_train\" to train the model using hashin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2814"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train.rdd.map(mapper)\n",
    "dataset.cache()  # cache dataset in RAM\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2091, 8877,    5,  375, 4901, 5024, 4841, 2903, 3936, 5200,   69,\n",
       "          166,  288,    8,    2, 4348,    0,   18,   21,    6,    1, 7010,\n",
       "         3462,  394,  217,  398,  407,  347,  448,    3,  483,  218, 1409,\n",
       "           19,  310,  557,   35,   40,   37,   29,  274,  191,  599,   22,\n",
       "          135,  100,  565, 1894,   95, 4281, 4864,  145,   49,    4,   83,\n",
       "          861,  105,  130,  107,    7,   27,   96,   17,  421, 4414,  126,\n",
       "          181,  202,  134, 2278,  490,  149,  376,  597,   15,  262, 6768,\n",
       "          843,   36,   25,   48,   10,  536,  179,  222]),\n",
       "  array([0.01709402, 0.03418803, 0.01709402, 0.00854701, 0.01709402,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.02564103, 0.01709402,\n",
       "         0.00854701, 0.00854701, 0.01709402, 0.05128205, 0.00854701,\n",
       "         0.05982906, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.02564103,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.01709402,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.02564103, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.02564103,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701,\n",
       "         0.00854701, 0.00854701, 0.00854701, 0.00854701, 0.00854701]),\n",
       "  0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2814"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of examples\n",
    "N = dataset.count()\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 499 loss: 0.4830229825164852\n"
     ]
    }
   ],
   "source": [
    "# random weights\n",
    "np.random.seed(0)\n",
    "weights = np.random.random(len(word_to_index))\n",
    "\n",
    "# Gradient Descent Epoch\n",
    "for i in range(500):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # calculate the gradient\n",
    "    gradient = (\n",
    "        dataset\n",
    "        .coalesce(2)  # merge 200 cached partitions into 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # update the weights\n",
    "    weights -= 0.4 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    \n",
    "    # print(\"epoch:\", i, \"loss:\", loss.value / N)\n",
    "print(\"epoch:\", i, \"loss:\", loss.value / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.34111336, -0.87491418, 14.74509065, ...,  1.35699863,\n",
       "         0.02378743,  1.19216271]),\n",
       " 10000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step 8: Calculate \"roc_auc_score\" for test dataset \"df_test\" using hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2785"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_test.rdd.map(mapper)\n",
    "dataset.cache()  # cache dataset in RAM\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8880199106275367, 1), (0.859675802580131, 1), (0.7565031406935169, 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocAuc_dataset = dataset.map(rocAuc)\n",
    "rocAuc_dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_score = rocAuc_dataset.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_true = rocAuc_dataset.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851919936196855"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stop Spark (and YARN application)\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"q1\": 0.57, \"q2\": 0.6851919936196855}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = { \"q1\": 0.57, \"q2\": 0.6851919936196855 }\n",
    "json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('result.json', 'w') as f:\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Correct q1 answer! Correct q2 answer!\n"
     ]
    }
   ],
   "source": [
    "! curl -F file=@result.json \"51.250.54.133:80/MDS-LSML1/aizmalkin/w3/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
