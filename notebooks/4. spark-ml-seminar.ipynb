{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkContext and SparkSession\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/sql-getting-started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-04-19 17:01:21,014 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-04-19 17:01:21,015 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2023-04-19 17:01:21,981 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 205M\n",
      "-rw-r--r-- 1 jovyan root  61M Oct 18 15:47 categories.jsonl\n",
      "-rw-r--r-- 1 jovyan root  387 Sep 26 22:14 README.txt\n",
      "-rw-r--r-- 1 jovyan root 144M Sep 26 23:25 wiki.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls -lh wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"April\", \"text\": \"April\\n\\nApril is the fourth month of the year, and comes between March and May. It is one of four months to have 30 days.\\n\\nApril always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\\n\\nApril's flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\\n\\nApril comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\\n\\nApril begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other's last days are exactly 35 weeks (245 days) apart.\\n\\nIn common years, April starts on the same day of the week as October of the previous year, and in leap years, May of the previous year. In common years, April finishes on the same day of the week as July of the previous year, and in leap years, February and October of the previous year. In common years immediately after other common years, April starts on the same day of the week as January of the previous year, and in leap years and years immediately after that, April finishes on the same day of the week as January of the previous year.\\n\\nIn years immediately before common years, April starts on the same day of the week as September and December of the following year, and in years immediately before leap years, June of the following year. In years immediately before common years, April finishes on the same day of the week as September of the following year, and in years immediately before leap years, March and June of the following year.\\n\\nApril is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\\n\\nIt is unclear as to where April got its name. A common theory is that it comes from the Latin word \\\"aperire\\\", meaning \\\"to open\\\", referring to flowers opening in spring. Another theory is that the name could come from Aphrodite, the Greek goddess of love. It was originally the second month in the old Roman Calendar, before the start of the new year was put to January 1.\\n\\nQuite a few festivals are held in this month. In many Southeast Asian cultures, new year is celebrated in this month (including Songkran). In Western Christianity, Easter can be celebrated on a Sunday between March 22 and April 25. In Orthodox Christianity, it can fall between April 4 and May 8. At the end of the month, Central and Northern European cultures celebrate Walpurgis Night on April 30, marking the transition from winter into summer.\\n\\nPoets use \\\"April\\\" to mean the end of winter. For example: \\\"April showers bring May flowers.\\\"\\n\\n\\n\", \"url\": \"https://simple.wikipedia.org/wiki?curid=1\", \"id\": \"1\"}\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 wiki/wiki.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"page_id\":1,\"category\":\"Months\"}\n",
      "{\"page_id\":2,\"category\":\"Months\"}\n",
      "{\"page_id\":6,\"category\":\"Art\"}\n",
      "{\"page_id\":6,\"category\":\"Basic_English_850_words\"}\n",
      "{\"page_id\":6,\"category\":\"CS1_Russian-language_sources_(ru)\"}\n"
     ]
    }
   ],
   "source": [
    "! head -n 5 wiki/categories.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal wiki /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 jovyan supergroup        387 2020-10-24 18:46 /wiki/README.txt\n",
      "-rw-r--r--   1 jovyan supergroup     60.9 M 2020-10-24 18:46 /wiki/categories.jsonl\n",
      "-rw-r--r--   1 jovyan supergroup    143.4 M 2020-10-24 18:46 /wiki/wiki.jsonl\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model for text vectorization\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "<img width=600px src='images/bow.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for text classification\n",
    "\n",
    "https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "<img width=600px src='images/lr.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dictionary of words (WordCount task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(f'[^{re.escape(string.printable)}]', ' ', text)  # replace unprintable characters with a space\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # and punctuation\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def mapper(line):\n",
    "    text = json.loads(line)['text']\n",
    "    words = tokenize(text)\n",
    "    return [(word, 1) for word in set(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 106 ms, sys: 19.5 ms, total: 125 ms\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = (\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .flatMap(mapper)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('continued', 2222),\n",
       " ('marguerite', 73),\n",
       " ('family', 7907),\n",
       " ('earl', 474),\n",
       " ('charge', 1236)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306564"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_counts = sorted(word_counts, key=lambda x: -x[1])[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 143585),\n",
       " ('in', 134975),\n",
       " ('a', 134315),\n",
       " ('of', 128899),\n",
       " ('is', 125063)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pals', 7),\n",
       " ('subgenera', 7),\n",
       " ('horwitz', 7),\n",
       " ('crocodylomorphs', 7),\n",
       " ('khabarovsk', 7)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes are needed for vectorization of texts\n",
    "word_to_index = {word: index for index, (word, count) in enumerate(top_word_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0), ('in', 1), ('a', 2), ('of', 3), ('is', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_index.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize texts (the \"bag of words\" model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([\"a\", \"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first option: the word_to_index dictionary will be serialized using pickle along with the function\n",
    "import numpy as np\n",
    "\n",
    "def mapper(line):\n",
    "    j = json.loads(line)\n",
    "    text = j['text']\n",
    "    words = tokenize(text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 338 ms, sys: 4.54 ms, total: 343 ms\n",
      "Wall time: 543 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([  119,     4,     0,   673,  1165,     3,   107,     5,   460,\n",
       "            82,   109,    57,     8,    27,   150,   654,     7,    37,\n",
       "           240,   459,   630,  1806,     9,   129,   222,  1185,    11,\n",
       "           115,  5341,    90,     1,  5614,    72,  1679,   131,    15,\n",
       "          2308,    21,  2576, 13008,  9614,    43, 32830,  4556,   575,\n",
       "         10878,   357,    20,    30,   117,    19,   120,   126,   132,\n",
       "            93,   352,   166,    41,   230,  2350,  1351,  1198, 11111,\n",
       "          1665,   315,  1266,   127,  1347,  7241,   136,  2294,    39,\n",
       "           106,   308,     2,  1516,   374,  3895,    13,  4121,  1021,\n",
       "           410,  7638,  3275,  6410,    89,   397,    78,  1174,    14,\n",
       "           833,   418,   628,  5464,  1619,   198,   227,   487, 13721,\n",
       "           718,  3263,   636,     6,   564,   108,   187,   611,  2482,\n",
       "           615,    53,   454,    64,  1259,   341,  4367,   353,    24,\n",
       "            44,  1184,  1720,  2828,  2845,   145,   354,  2572,  6066,\n",
       "            47,    26,  2633,   343,   272,  3283,   112,   194,    16,\n",
       "           253,   326,   491,  3891,   663,  8789,  5006,   902,    67,\n",
       "           523,  4795,   148,  1100,    10,   250, 15943,  1628]),\n",
       "  array([ 0.03954802,  0.01883239,  0.10734463,  0.00376648,  0.01506591,\n",
       "          0.05838041,  0.03389831,  0.03766478,  0.00753296,  0.00753296,\n",
       "          0.00753296,  0.0094162 ,  0.01506591,  0.00188324,  0.00376648,\n",
       "          0.00376648,  0.01129944,  0.00376648,  0.00564972,  0.00753296,\n",
       "          0.00376648,  0.00376648,  0.02448211,  0.02071563,  0.02071563,\n",
       "          0.02071563,  0.02636535,  0.00564972,  0.00188324,  0.0094162 ,\n",
       "          0.04708098,  0.01318267,  0.03389831,  0.00376648,  0.00564972,\n",
       "          0.00376648,  0.00564972,  0.00753296,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00376648,  0.00376648,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00753296,  0.00564972,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00376648,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.01318267,\n",
       "          0.00564972,  0.00564972,  0.01129944,  0.00564972,  0.00188324,\n",
       "          0.01129944,  0.00376648,  0.0094162 ,  0.00753296,  0.00753296,\n",
       "          0.00376648,  0.00376648,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00376648,  0.00564972,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00376648,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00376648,  0.00188324,  0.00188324,  0.00376648,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .map(mapper)\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second option: broadcast variable\n",
    "word_to_index_broadcast = sc.broadcast(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables are useful when you want to broadcast the same data to all executors:\n",
    "- dictionary in ML algorithm\n",
    "- vector of weights in ML algorithm\n",
    "\n",
    "Executors have **read-only** access to this data\n",
    "\n",
    "Send once and can be used multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    j = json.loads(line)\n",
    "    text = j['text']\n",
    "    words = tokenize(text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index_broadcast.value:\n",
    "            index = word_to_index_broadcast.value[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.62 ms, sys: 0 ns, total: 6.62 ms\n",
      "Wall time: 163 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([  119,     4,     0,   673,  1165,     3,   107,     5,   460,\n",
       "            82,   109,    57,     8,    27,   150,   654,     7,    37,\n",
       "           240,   459,   630,  1806,     9,   129,   222,  1185,    11,\n",
       "           115,  5341,    90,     1,  5614,    72,  1679,   131,    15,\n",
       "          2308,    21,  2576, 13008,  9614,    43, 32830,  4556,   575,\n",
       "         10878,   357,    20,    30,   117,    19,   120,   126,   132,\n",
       "            93,   352,   166,    41,   230,  2350,  1351,  1198, 11111,\n",
       "          1665,   315,  1266,   127,  1347,  7241,   136,  2294,    39,\n",
       "           106,   308,     2,  1516,   374,  3895,    13,  4121,  1021,\n",
       "           410,  7638,  3275,  6410,    89,   397,    78,  1174,    14,\n",
       "           833,   418,   628,  5464,  1619,   198,   227,   487, 13721,\n",
       "           718,  3263,   636,     6,   564,   108,   187,   611,  2482,\n",
       "           615,    53,   454,    64,  1259,   341,  4367,   353,    24,\n",
       "            44,  1184,  1720,  2828,  2845,   145,   354,  2572,  6066,\n",
       "            47,    26,  2633,   343,   272,  3283,   112,   194,    16,\n",
       "           253,   326,   491,  3891,   663,  8789,  5006,   902,    67,\n",
       "           523,  4795,   148,  1100,    10,   250, 15943,  1628]),\n",
       "  array([ 0.03954802,  0.01883239,  0.10734463,  0.00376648,  0.01506591,\n",
       "          0.05838041,  0.03389831,  0.03766478,  0.00753296,  0.00753296,\n",
       "          0.00753296,  0.0094162 ,  0.01506591,  0.00188324,  0.00376648,\n",
       "          0.00376648,  0.01129944,  0.00376648,  0.00564972,  0.00753296,\n",
       "          0.00376648,  0.00376648,  0.02448211,  0.02071563,  0.02071563,\n",
       "          0.02071563,  0.02636535,  0.00564972,  0.00188324,  0.0094162 ,\n",
       "          0.04708098,  0.01318267,  0.03389831,  0.00376648,  0.00564972,\n",
       "          0.00376648,  0.00564972,  0.00753296,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00376648,  0.00376648,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00753296,  0.00564972,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00376648,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.01318267,\n",
       "          0.00564972,  0.00564972,  0.01129944,  0.00564972,  0.00188324,\n",
       "          0.01129944,  0.00376648,  0.0094162 ,  0.00753296,  0.00753296,\n",
       "          0.00376648,  0.00376648,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00376648,  0.00564972,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00376648,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00376648,  0.00188324,  0.00188324,  0.00376648,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .map(mapper)\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third option: save to file yourself\n",
    "import pickle\n",
    "pickle.dump(word_to_index, open(\"word_to_index.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(lines):\n",
    "    word_to_index_loaded = pickle.load(open(\"word_to_index.pickle\", 'rb'))\n",
    "    for line in lines:\n",
    "        j = json.loads(line)\n",
    "        text = j['text']\n",
    "        words = tokenize(text)\n",
    "        indices = []\n",
    "        values = []\n",
    "        for word, count in Counter(words).items():\n",
    "            if word in word_to_index_loaded:\n",
    "                index = word_to_index_loaded[word]\n",
    "                indices.append(index)\n",
    "                tf = count / float(len(words))\n",
    "                values.append(tf)\n",
    "        yield np.array(indices), np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"word_to_index.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 ms, sys: 1.65 ms, total: 6.37 ms\n",
      "Wall time: 206 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([  119,     4,     0,   673,  1165,     3,   107,     5,   460,\n",
       "            82,   109,    57,     8,    27,   150,   654,     7,    37,\n",
       "           240,   459,   630,  1806,     9,   129,   222,  1185,    11,\n",
       "           115,  5341,    90,     1,  5614,    72,  1679,   131,    15,\n",
       "          2308,    21,  2576, 13008,  9614,    43, 32830,  4556,   575,\n",
       "         10878,   357,    20,    30,   117,    19,   120,   126,   132,\n",
       "            93,   352,   166,    41,   230,  2350,  1351,  1198, 11111,\n",
       "          1665,   315,  1266,   127,  1347,  7241,   136,  2294,    39,\n",
       "           106,   308,     2,  1516,   374,  3895,    13,  4121,  1021,\n",
       "           410,  7638,  3275,  6410,    89,   397,    78,  1174,    14,\n",
       "           833,   418,   628,  5464,  1619,   198,   227,   487, 13721,\n",
       "           718,  3263,   636,     6,   564,   108,   187,   611,  2482,\n",
       "           615,    53,   454,    64,  1259,   341,  4367,   353,    24,\n",
       "            44,  1184,  1720,  2828,  2845,   145,   354,  2572,  6066,\n",
       "            47,    26,  2633,   343,   272,  3283,   112,   194,    16,\n",
       "           253,   326,   491,  3891,   663,  8789,  5006,   902,    67,\n",
       "           523,  4795,   148,  1100,    10,   250, 15943,  1628]),\n",
       "  array([ 0.03954802,  0.01883239,  0.10734463,  0.00376648,  0.01506591,\n",
       "          0.05838041,  0.03389831,  0.03766478,  0.00753296,  0.00753296,\n",
       "          0.00753296,  0.0094162 ,  0.01506591,  0.00188324,  0.00376648,\n",
       "          0.00376648,  0.01129944,  0.00376648,  0.00564972,  0.00753296,\n",
       "          0.00376648,  0.00376648,  0.02448211,  0.02071563,  0.02071563,\n",
       "          0.02071563,  0.02636535,  0.00564972,  0.00188324,  0.0094162 ,\n",
       "          0.04708098,  0.01318267,  0.03389831,  0.00376648,  0.00564972,\n",
       "          0.00376648,  0.00564972,  0.00753296,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00376648,  0.00376648,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00753296,  0.00564972,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00376648,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.01318267,\n",
       "          0.00564972,  0.00564972,  0.01129944,  0.00564972,  0.00188324,\n",
       "          0.01129944,  0.00376648,  0.0094162 ,  0.00753296,  0.00753296,\n",
       "          0.00376648,  0.00376648,  0.00564972,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00376648,  0.00564972,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00376648,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00376648,  0.00188324,  0.00188324,  0.00376648,\n",
       "          0.00188324,  0.00376648,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00376648,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00376648,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324,  0.00188324,  0.00188324,  0.00188324,\n",
       "          0.00188324,  0.00188324]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .mapPartitions(mapper)\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We consider the coverage of words by a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulator variables\n",
    "all_words = sc.accumulator(0)\n",
    "covered_words = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulator variables are useful when you need a counter (adder) of events on executors:\n",
    "- number of unknown words\n",
    "- number of corrupted records\n",
    "\n",
    "Only the driver can read the totals.\n",
    "\n",
    "For executors, access to the counter is **write-only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    global all_words\n",
    "    global covered_words\n",
    "    j = json.loads(line)\n",
    "    text = j['text']\n",
    "    words = tokenize(text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        all_words += count\n",
    "        if word in word_to_index:\n",
    "            covered_words += count\n",
    "            index = word_to_index[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 330 ms, sys: 0 ns, total: 330 ms\n",
      "Wall time: 23.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154259"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .map(mapper)\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21603362\n",
      "22321968\n"
     ]
    }
   ],
   "source": [
    "print(covered_words.value)\n",
    "print(all_words.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage: 0.9678072291833767\n"
     ]
    }
   ],
   "source": [
    "print(\"coverage:\", covered_words.value / float(all_words.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File with article categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April\\n\\nApril is the fourth month of the year...</td>\n",
       "      <td>April</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August\\n\\nAugust (Aug.) is the eighth month of...</td>\n",
       "      <td>August</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text   title  \\\n",
       "0  1  April\\n\\nApril is the fourth month of the year...   April   \n",
       "1  2  August\\n\\nAugust (Aug.) is the eighth month of...  August   \n",
       "\n",
       "                                         url  \n",
       "0  https://simple.wikipedia.org/wiki?curid=1  \n",
       "1  https://simple.wikipedia.org/wiki?curid=2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = se.read.json(\"hdfs:///wiki/wiki.jsonl\")\n",
    "wiki.registerTempTable(\"wiki\")\n",
    "wiki.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Months</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Months</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  page_id\n",
       "0   Months        1\n",
       "1   Months        2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = se.read.json(\"hdfs:///wiki/categories.jsonl\")\n",
    "categories.registerTempTable(\"categories\")\n",
    "categories.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Living_people</td>\n",
       "      <td>23886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Commons_category_link_is_on_Wikidata</td>\n",
       "      <td>18754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pages_translated_from_English_Wikipedia</td>\n",
       "      <td>16860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coordinates_on_Wikidata</td>\n",
       "      <td>16084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>People_stubs</td>\n",
       "      <td>14828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Articles_with_hCards</td>\n",
       "      <td>13823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>United_States_geography_stubs</td>\n",
       "      <td>12607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Commons_category_link_from_Wikidata</td>\n",
       "      <td>9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>American_people_stubs</td>\n",
       "      <td>9689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sportspeople_stubs</td>\n",
       "      <td>8749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>France_geography_stubs</td>\n",
       "      <td>8692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Geography_stubs</td>\n",
       "      <td>7216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pages_with_maps</td>\n",
       "      <td>7179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Europe_stubs</td>\n",
       "      <td>6817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stubs</td>\n",
       "      <td>5244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Template_documentation_pages</td>\n",
       "      <td>4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Articles_with_'species'_microformats</td>\n",
       "      <td>4304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Noindexed_pages</td>\n",
       "      <td>4280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>American_movie_actors</td>\n",
       "      <td>4035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Movie_stubs</td>\n",
       "      <td>3885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Biology_stubs</td>\n",
       "      <td>3871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>3807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Music_stubs</td>\n",
       "      <td>3787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>All_disambiguation_pages</td>\n",
       "      <td>3743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>All_article_disambiguation_pages</td>\n",
       "      <td>3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Taxobox_articles_missing_a_taxonbar</td>\n",
       "      <td>3697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Disambiguation_pages</td>\n",
       "      <td>3646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>American_actor_stubs</td>\n",
       "      <td>3626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Infobox_mapframe_without_OSM_relation_ID_on_Wi...</td>\n",
       "      <td>3517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Asia_stubs</td>\n",
       "      <td>3456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Wikipedia_articles_with_VIAF_identifiers</td>\n",
       "      <td>3378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Wikipedia_articles_with_LCCN_identifiers</td>\n",
       "      <td>3201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Switzerland_stubs</td>\n",
       "      <td>3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Articles_containing_Japanese-language_text</td>\n",
       "      <td>3122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Wikipedia_articles_with_WorldCat_identifiers</td>\n",
       "      <td>3102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020_deaths</td>\n",
       "      <td>3081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Hidden_categories</td>\n",
       "      <td>3042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Wikipedia_articles_with_GND_identifiers</td>\n",
       "      <td>2978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Requests_for_deletion_that_succeeded</td>\n",
       "      <td>2884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CS1_errors:_dates</td>\n",
       "      <td>2848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>All_articles_lacking_sources</td>\n",
       "      <td>2834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Actor_stubs</td>\n",
       "      <td>2771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>United_Kingdom_stubs</td>\n",
       "      <td>2744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sports_stubs</td>\n",
       "      <td>2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Shared_IP_addresses_from_schools</td>\n",
       "      <td>2644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Articles_with_hAudio_microformats</td>\n",
       "      <td>2578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CatAutoTOC_generates_no_TOC</td>\n",
       "      <td>2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Taxonomy_templates</td>\n",
       "      <td>2431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Transport_stubs</td>\n",
       "      <td>2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Articles_with_French-language_sources_(fr)</td>\n",
       "      <td>2392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Wikipedia_articles_with_ISNI_identifiers</td>\n",
       "      <td>2371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2019_deaths</td>\n",
       "      <td>2347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2017_deaths</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Japanese_sportspeople_stubs</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Articles_with_German-language_sources_(de)</td>\n",
       "      <td>2134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>English-language_movies</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Pages_using_infobox_football_biography_with_po...</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Wikipedia_articles_with_BNF_identifiers</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018_deaths</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Technology_stubs</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>CS1_Swedish-language_sources_(sv)</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>United_States_stubs</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Wikipedia_articles_with_NKC_identifiers</td>\n",
       "      <td>1805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>All_pages_that_need_simplifying</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>American_voice_actors</td>\n",
       "      <td>1733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>All_articles_with_unsourced_statements</td>\n",
       "      <td>1717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2014_deaths</td>\n",
       "      <td>1660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Templates_using_TemplateData</td>\n",
       "      <td>1649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Wikipedia_articles_with_SUDOC_identifiers</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Pages_using_infobox_person_with_unknown_parame...</td>\n",
       "      <td>1630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>American_stage_actors</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Science_stubs</td>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>CS1_German-language_sources_(de)</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Category_redirects</td>\n",
       "      <td>1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2015_deaths</td>\n",
       "      <td>1479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Religion_stubs</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>CS1_French-language_sources_(fr)</td>\n",
       "      <td>1469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Biography_articles_of_living_people</td>\n",
       "      <td>1433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Pages_using_deprecated_image_syntax</td>\n",
       "      <td>1432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>All_orphaned_articles</td>\n",
       "      <td>1419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Politics_stubs</td>\n",
       "      <td>1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Pages_using_infobox_settlement_with_unknown_pa...</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Television_stubs</td>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Taxonomy_templates_with_red-linked_taxa</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>CS1_Spanish-language_sources_(es)</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>History_stubs</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2016_deaths</td>\n",
       "      <td>1318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Monthly_clean-up_category_counter</td>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Canada_stubs</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Webarchive_template_wayback_links</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>CS1_maint:_multiple_names:_authors_list</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Wikipedia_articles_with_NTA_identifiers</td>\n",
       "      <td>1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>US_Democratic_Party_politicians</td>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Wikipedia_articles_with_NDL_identifiers</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Japan_stubs</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Wikipedia_Did_you_know_articles</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Wikipedia_template_categories</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Deaths_from_myocardial_infarction</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Literature_stubs</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Articles_with_Japanese-language_sources_(ja)</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             category    cnt\n",
       "0                                       Living_people  23886\n",
       "1                Commons_category_link_is_on_Wikidata  18754\n",
       "2             Pages_translated_from_English_Wikipedia  16860\n",
       "3                             Coordinates_on_Wikidata  16084\n",
       "4                                        People_stubs  14828\n",
       "5                                Articles_with_hCards  13823\n",
       "6                       United_States_geography_stubs  12607\n",
       "7                 Commons_category_link_from_Wikidata   9825\n",
       "8                               American_people_stubs   9689\n",
       "9                                  Sportspeople_stubs   8749\n",
       "10                             France_geography_stubs   8692\n",
       "11                                    Geography_stubs   7216\n",
       "12                                    Pages_with_maps   7179\n",
       "13                                       Europe_stubs   6817\n",
       "14                                              Stubs   5244\n",
       "15                       Template_documentation_pages   4314\n",
       "16               Articles_with_'species'_microformats   4304\n",
       "17                                    Noindexed_pages   4280\n",
       "18                              American_movie_actors   4035\n",
       "19                                        Movie_stubs   3885\n",
       "20                                      Biology_stubs   3871\n",
       "21                         American_television_actors   3807\n",
       "22                                        Music_stubs   3787\n",
       "23                           All_disambiguation_pages   3743\n",
       "24                   All_article_disambiguation_pages   3739\n",
       "25                Taxobox_articles_missing_a_taxonbar   3697\n",
       "26                               Disambiguation_pages   3646\n",
       "27                               American_actor_stubs   3626\n",
       "28  Infobox_mapframe_without_OSM_relation_ID_on_Wi...   3517\n",
       "29                                         Asia_stubs   3456\n",
       "30           Wikipedia_articles_with_VIAF_identifiers   3378\n",
       "31           Wikipedia_articles_with_LCCN_identifiers   3201\n",
       "32                                  Switzerland_stubs   3198\n",
       "33         Articles_containing_Japanese-language_text   3122\n",
       "34       Wikipedia_articles_with_WorldCat_identifiers   3102\n",
       "35                                        2020_deaths   3081\n",
       "36                                  Hidden_categories   3042\n",
       "37            Wikipedia_articles_with_GND_identifiers   2978\n",
       "38               Requests_for_deletion_that_succeeded   2884\n",
       "39                                  CS1_errors:_dates   2848\n",
       "40                       All_articles_lacking_sources   2834\n",
       "41                                        Actor_stubs   2771\n",
       "42                               United_Kingdom_stubs   2744\n",
       "43                                       Sports_stubs   2724\n",
       "44                   Shared_IP_addresses_from_schools   2644\n",
       "45                  Articles_with_hAudio_microformats   2578\n",
       "46                        CatAutoTOC_generates_no_TOC   2551\n",
       "47                                 Taxonomy_templates   2431\n",
       "48                                    Transport_stubs   2422\n",
       "49         Articles_with_French-language_sources_(fr)   2392\n",
       "50           Wikipedia_articles_with_ISNI_identifiers   2371\n",
       "51                                        2019_deaths   2347\n",
       "52                                        2017_deaths   2259\n",
       "53                        Japanese_sportspeople_stubs   2186\n",
       "54         Articles_with_German-language_sources_(de)   2134\n",
       "55                            English-language_movies   2108\n",
       "56  Pages_using_infobox_football_biography_with_po...   2098\n",
       "57            Wikipedia_articles_with_BNF_identifiers   2057\n",
       "58                                        2018_deaths   2003\n",
       "59                                   Technology_stubs   1955\n",
       "60                  CS1_Swedish-language_sources_(sv)   1867\n",
       "61                                United_States_stubs   1858\n",
       "62            Wikipedia_articles_with_NKC_identifiers   1805\n",
       "63                    All_pages_that_need_simplifying   1790\n",
       "64                              American_voice_actors   1733\n",
       "65             All_articles_with_unsourced_statements   1717\n",
       "66                                        2014_deaths   1660\n",
       "67                       Templates_using_TemplateData   1649\n",
       "68          Wikipedia_articles_with_SUDOC_identifiers   1635\n",
       "69  Pages_using_infobox_person_with_unknown_parame...   1630\n",
       "70                              American_stage_actors   1597\n",
       "71                                      Science_stubs   1526\n",
       "72                   CS1_German-language_sources_(de)   1520\n",
       "73                                 Category_redirects   1487\n",
       "74                                        2015_deaths   1479\n",
       "75                                     Religion_stubs   1478\n",
       "76                   CS1_French-language_sources_(fr)   1469\n",
       "77                Biography_articles_of_living_people   1433\n",
       "78                Pages_using_deprecated_image_syntax   1432\n",
       "79                              All_orphaned_articles   1419\n",
       "80                                     Politics_stubs   1417\n",
       "81  Pages_using_infobox_settlement_with_unknown_pa...   1380\n",
       "82                                   Television_stubs   1361\n",
       "83            Taxonomy_templates_with_red-linked_taxa   1351\n",
       "84                  CS1_Spanish-language_sources_(es)   1339\n",
       "85                                      History_stubs   1336\n",
       "86                                        2016_deaths   1318\n",
       "87                  Monthly_clean-up_category_counter   1314\n",
       "88                                       Canada_stubs   1307\n",
       "89                  Webarchive_template_wayback_links   1300\n",
       "90            CS1_maint:_multiple_names:_authors_list   1291\n",
       "91            Wikipedia_articles_with_NTA_identifiers   1264\n",
       "92                    US_Democratic_Party_politicians   1263\n",
       "93            Wikipedia_articles_with_NDL_identifiers   1228\n",
       "94                                        Japan_stubs   1203\n",
       "95                    Wikipedia_Did_you_know_articles   1196\n",
       "96                      Wikipedia_template_categories   1187\n",
       "97                  Deaths_from_myocardial_infarction   1181\n",
       "98                                   Literature_stubs   1170\n",
       "99       Articles_with_Japanese-language_sources_(ja)   1167"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "\n",
    "se.sql(\"\"\"\n",
    "select \n",
    "    category,\n",
    "    count(*) as cnt\n",
    "from categories\n",
    "group by category\n",
    "order by cnt desc\n",
    "\"\"\").limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=474</td>\n",
       "      <td>Articles_containing_French-language_text</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>474</td>\n",
       "      <td>Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=474</td>\n",
       "      <td>CS1_French-language_sources_(fr)</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=474</td>\n",
       "      <td>Coordinates_on_Wikidata</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>474</td>\n",
       "      <td>Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=474</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>474</td>\n",
       "      <td>Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=474</td>\n",
       "      <td>Olympic_cities</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text     title  \\\n",
       "0  474  Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...  Montreal   \n",
       "1  474  Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...  Montreal   \n",
       "2  474  Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...  Montreal   \n",
       "3  474  Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...  Montreal   \n",
       "4  474  Montreal\\n\\nMontreal (, spelled \"Montréal\" in ...  Montreal   \n",
       "\n",
       "                                           url  \\\n",
       "0  https://simple.wikipedia.org/wiki?curid=474   \n",
       "1  https://simple.wikipedia.org/wiki?curid=474   \n",
       "2  https://simple.wikipedia.org/wiki?curid=474   \n",
       "3  https://simple.wikipedia.org/wiki?curid=474   \n",
       "4  https://simple.wikipedia.org/wiki?curid=474   \n",
       "\n",
       "                                   category  page_id  \n",
       "0  Articles_containing_French-language_text      474  \n",
       "1          CS1_French-language_sources_(fr)      474  \n",
       "2                   Coordinates_on_Wikidata      474  \n",
       "3                                  Montreal      474  \n",
       "4                            Olympic_cities      474  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple join\n",
    "joined = se.sql(\"\"\"\n",
    "select\n",
    "    *\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "\"\"\")\n",
    "joined.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [cast(id#7 as bigint)], [page_id#27L], Inner\n",
      ":- *(2) Sort [cast(id#7 as bigint) ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(cast(id#7 as bigint), 200), true, [id=#127]\n",
      ":     +- *(1) Project [id#7, text#8, title#9, url#10]\n",
      ":        +- *(1) Filter isnotnull(id#7)\n",
      ":           +- FileScan json [id#7,text#8,title#9,url#10] Batched: false, DataFilters: [isnotnull(id#7)], Format: JSON, Location: InMemoryFileIndex[hdfs://localhost:9000/wiki/wiki.jsonl], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,text:string,title:string,url:string>\n",
      "+- *(4) Sort [page_id#27L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(page_id#27L, 200), true, [id=#136]\n",
      "      +- *(3) Project [category#26, page_id#27L]\n",
      "         +- *(3) Filter isnotnull(page_id#27L)\n",
      "            +- FileScan json [category#26,page_id#27L] Batched: false, DataFilters: [isnotnull(page_id#27L)], Format: JSON, Location: InMemoryFileIndex[hdfs://localhost:9000/wiki/categories.jsonl], PartitionFilters: [], PushedFilters: [IsNotNull(page_id)], ReadSchema: struct<category:string,page_id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April\\n\\nApril is the fourth month of the year...</td>\n",
       "      <td>April</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=1</td>\n",
       "      <td>Months</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August\\n\\nAugust (Aug.) is the eighth month of...</td>\n",
       "      <td>August</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=2</td>\n",
       "      <td>Months</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Art\\n\\nArt is a creative activity that express...</td>\n",
       "      <td>Art</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=6</td>\n",
       "      <td>Pages_using_ISBN_magic_links</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Art\\n\\nArt is a creative activity that express...</td>\n",
       "      <td>Art</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=6</td>\n",
       "      <td>Non-verbal_communication</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Art\\n\\nArt is a creative activity that express...</td>\n",
       "      <td>Art</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=6</td>\n",
       "      <td>Commons_category_link_from_Wikidata</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text   title  \\\n",
       "0  1  April\\n\\nApril is the fourth month of the year...   April   \n",
       "1  2  August\\n\\nAugust (Aug.) is the eighth month of...  August   \n",
       "2  6  Art\\n\\nArt is a creative activity that express...     Art   \n",
       "3  6  Art\\n\\nArt is a creative activity that express...     Art   \n",
       "4  6  Art\\n\\nArt is a creative activity that express...     Art   \n",
       "\n",
       "                                         url  \\\n",
       "0  https://simple.wikipedia.org/wiki?curid=1   \n",
       "1  https://simple.wikipedia.org/wiki?curid=2   \n",
       "2  https://simple.wikipedia.org/wiki?curid=6   \n",
       "3  https://simple.wikipedia.org/wiki?curid=6   \n",
       "4  https://simple.wikipedia.org/wiki?curid=6   \n",
       "\n",
       "                              category  page_id  \n",
       "0                               Months        1  \n",
       "1                               Months        2  \n",
       "2         Pages_using_ISBN_magic_links        6  \n",
       "3             Non-verbal_communication        6  \n",
       "4  Commons_category_link_from_Wikidata        6  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcast join (https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#join-hints)\n",
    "joined = se.sql(\"\"\"\n",
    "select /*+ BROADCAST(categories) */\n",
    "    *\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "\"\"\")\n",
    "joined.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [cast(id#7 as bigint)], [page_id#27L], Inner, BuildRight\n",
      ":- *(2) Project [id#7, text#8, title#9, url#10]\n",
      ":  +- *(2) Filter isnotnull(id#7)\n",
      ":     +- FileScan json [id#7,text#8,title#9,url#10] Batched: false, DataFilters: [isnotnull(id#7)], Format: JSON, Location: InMemoryFileIndex[hdfs://localhost:9000/wiki/wiki.jsonl], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,text:string,title:string,url:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true])), [id=#208]\n",
      "   +- *(1) Project [category#26, page_id#27L]\n",
      "      +- *(1) Filter isnotnull(page_id#27L)\n",
      "         +- FileScan json [category#26,page_id#27L] Batched: false, DataFilters: [isnotnull(page_id#27L)], Format: JSON, Location: InMemoryFileIndex[hdfs://localhost:9000/wiki/categories.jsonl], PartitionFilters: [], PushedFilters: [IsNotNull(page_id)], ReadSchema: struct<category:string,page_id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewed Key problem by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.tokenize(text)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.udf.register(\"tokenize\", tokenize, \"array<string>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Living_people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cent</td>\n",
       "      <td>Living_people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token       category\n",
       "0    50  Living_people\n",
       "1  cent  Living_people"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = se.sql(\"\"\"\n",
    "select\n",
    "    explode(tokenize(wiki.text)) as token,\n",
    "    categories.category\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('Living_people', 'Movies_based_on_books')\n",
    "\"\"\")\n",
    "joined.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.write.parquet(\"hdfs:///token_category.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|   token|     category|\n",
      "+--------+-------------+\n",
      "|   sarah|Living_people|\n",
      "|michelle|Living_people|\n",
      "+--------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.read.parquet(\"hdfs:///token_category.parquet\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"token_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            category|    cnt|\n",
      "+--------------------+-------+\n",
      "|Movies_based_on_b...|  89520|\n",
      "|       Living_people|2468253|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.sql(\"\"\"\n",
    "select \n",
    "    category,\n",
    "    count(*) as cnt\n",
    "from token_category\n",
    "group by category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Movies_based_on_books', 89520), ('Living_people', 2468253)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import random\n",
    "\n",
    "def slow_count(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        count += 1\n",
    "        # slow down to show\n",
    "        if random.random() < 0.1:\n",
    "            time.sleep(0.00000001)\n",
    "    return count\n",
    "\n",
    "(\n",
    "    df.rdd\n",
    "    .map(lambda x: (x.category, x.token))\n",
    "    .groupByKey(numPartitions=10)\n",
    "    .map(lambda x: (x[0], slow_count(x[1])))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Living_people', 1), 246179),\n",
       " (('Living_people', 2), 245763),\n",
       " (('Living_people', 3), 247088),\n",
       " (('Living_people', 4), 247824),\n",
       " (('Living_people', 5), 247187),\n",
       " (('Living_people', 6), 246583),\n",
       " (('Living_people', 7), 246985),\n",
       " (('Living_people', 8), 247068),\n",
       " (('Living_people', 9), 247010),\n",
       " (('Living_people', 10), 246566),\n",
       " (('Movies_based_on_books', 1), 9014),\n",
       " (('Movies_based_on_books', 2), 9019),\n",
       " (('Movies_based_on_books', 3), 8927),\n",
       " (('Movies_based_on_books', 4), 8873),\n",
       " (('Movies_based_on_books', 5), 8981),\n",
       " (('Movies_based_on_books', 6), 8899),\n",
       " (('Movies_based_on_books', 7), 8973),\n",
       " (('Movies_based_on_books', 8), 8814),\n",
       " (('Movies_based_on_books', 9), 9016),\n",
       " (('Movies_based_on_books', 10), 9004)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sorted((\n",
    "    df.rdd\n",
    "    .map(lambda x: ((x.category, random.randint(1, 10)), x.token))  # made a dummy key\n",
    "    .groupByKey(numPartitions=10)\n",
    "    .map(lambda x: (x[0], slow_count(x[1])))\n",
    ").collect())  # it remains only to sum the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50 Cent\\n\\n50 Cent (also known as Fitty\" or \"F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Danny Jacobs (actor)\\n\\nDaniel Charles Jacobs,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  50 Cent\\n\\n50 Cent (also known as Fitty\" or \"F...       1\n",
       "1  Danny Jacobs (actor)\\n\\nDaniel Charles Jacobs,...       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = se.sql(\"\"\"\n",
    "select\n",
    "    wiki.text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where categories.category in ('American_movie_actors', 'US_Democratic_Party_politicians')\n",
    "\"\"\")\n",
    "joined.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5260"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = joined.rdd.map(mapper)\n",
    "dataset.cache()  # cache dataset in RAM\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  531,  7750,    20,    33,    11,    25,  3244,     4,    13,\n",
       "            35,  4850,   733,  2384,  2618,  2669,   287,  4920,     5,\n",
       "          1190,   737,    18,     6,    22,     1,  4114,    53,   225,\n",
       "            62,     9,   115,   164,   778,    28,   388,    78,    42,\n",
       "          1171,  6981,  1766,  1438,    75,   517,  2089,   338,   481,\n",
       "          3066,    51,   313,   194,    72,   187,   802,   413,     2,\n",
       "          3177,    45, 45674,  1739,   324,     7,   365,  1677,   208,\n",
       "           397,  1272,   886,   226,   106,  2135,  9909,    30,    73,\n",
       "           704,   236,   445,   648,    12,  1928,  4292,  5057,   201,\n",
       "           303,     0,   129,   271,    48,   190,   954,  1321,   100,\n",
       "            94,    23,   613,  2840,     3,   712,   381,   270,  1093,\n",
       "          1275,  1468,   317,  2899,  3988,     8,   182,    27,   440,\n",
       "           468,  1493,   793, 23173,   188,  1530,   150,   459,   133,\n",
       "           134,   335,   963,  1805,    10,  1169,  9657,   102,   523,\n",
       "           344,    54,   990,  6772,    17,   444,   554,  1357,   158,\n",
       "            39,    24,   263,    40,    63, 27991,   584,    50,  8217,\n",
       "            87,  1822,  8857,   430,  1240,  6790,  3149,  1906,  1908,\n",
       "          2756,  3466,    82, 12853,    15, 19614,  1371, 15728,  9518,\n",
       "          1108,  7994,    44,  2377,   320,  6455,  3334,   337,   998,\n",
       "          2833,  1408,  4277,   372,   922, 20422, 21301,   111,    14,\n",
       "           615, 16512,    26, 12325, 21541,  4082,   688,    95,  1952,\n",
       "           593,    21,   846,   166,   107,    77,   595,  1436,   961,\n",
       "          1718,    60,   856,   264,   475,    69,  4035,   296,   241,\n",
       "          5856,   178,    55,  1217,  2423,  2556,  1686,  2019, 32988,\n",
       "          1049,   380]),\n",
       "  array([ 0.03465347,  0.03217822,  0.00990099,  0.00247525,  0.0049505 ,\n",
       "          0.00742574,  0.00247525,  0.00990099,  0.0049505 ,  0.0049505 ,\n",
       "          0.0049505 ,  0.00742574,  0.00247525,  0.00247525,  0.00990099,\n",
       "          0.00247525,  0.00247525,  0.02475248,  0.00247525,  0.00247525,\n",
       "          0.02970297,  0.02475248,  0.00247525,  0.02227723,  0.00247525,\n",
       "          0.00742574,  0.00247525,  0.00247525,  0.01485149,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.02227723,  0.00247525,  0.0049505 ,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.0049505 ,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.02227723,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00742574,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.01237624,  0.00247525,  0.00247525,  0.00990099,  0.00247525,\n",
       "          0.0049505 ,  0.00990099,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.0049505 ,  0.04455446,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00742574,  0.0049505 ,  0.00247525,  0.01485149,\n",
       "          0.00742574,  0.00742574,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00742574,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.01237624,  0.00742574,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.0049505 ,  0.00990099,  0.00742574,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00990099,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00990099,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.0049505 ,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.0049505 ,  0.00247525,  0.00247525,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.0049505 ,  0.00247525,  0.00247525,\n",
       "          0.00247525,  0.00247525,  0.00247525]),\n",
       "  1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1. + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(weights_broadcast, loss, examples):\n",
    "    # here we accumulate the contribution to the gradient\n",
    "    gradient = np.zeros(len(weights_broadcast.value))\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # make a prediction with the current weights\n",
    "        p = sigmoid(values.dot(weights_broadcast.value[indices]))\n",
    "\n",
    "        # add to gradient accumulator\n",
    "        gradient[indices] += values * (p - target)\n",
    "\n",
    "        # count losses\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        loss.add(-(target * np.log(p) + (1 - target) * np.log(1 - p)))\n",
    "    \n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of examples\n",
    "N = dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.58625134286\n",
      "epoch: 1 loss: 0.527839043397\n",
      "epoch: 2 loss: 0.495466974821\n",
      "epoch: 3 loss: 0.47081708081\n",
      "epoch: 4 loss: 0.449695793506\n",
      "epoch: 5 loss: 0.430857922738\n",
      "epoch: 6 loss: 0.413814406772\n",
      "epoch: 7 loss: 0.398299721855\n",
      "epoch: 8 loss: 0.384125692498\n",
      "epoch: 9 loss: 0.371138697296\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# random weights\n",
    "weights = np.random.random(len(word_to_index))\n",
    "\n",
    "# Gradient Descent Epoch\n",
    "for i in range(10):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # calculate the gradient\n",
    "    gradient = (\n",
    "        dataset\n",
    "        .coalesce(2)  # merge 200 cached partitions into 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # update the weights\n",
    "    weights -= 0.05 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    \n",
    "    print(\"epoch:\", i, \"loss:\", loss.value / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11.100239587965177, 'in'),\n",
       " (7.6522764351610197, 'and'),\n",
       " (5.7339620979988899, 'she'),\n",
       " (5.3439673788711248, 'actor'),\n",
       " (4.3887261601815677, 'known'),\n",
       " (4.2384335148585421, 'born'),\n",
       " (3.9450777265003576, 'actress'),\n",
       " (3.8833970934648274, 'movie'),\n",
       " (3.4859693374264453, 'television'),\n",
       " (3.3010514645270699, 'is'),\n",
       " (3.0167908274737556, 'for'),\n",
       " (2.9851047406444557, 'her'),\n",
       " (2.939186703679157, 'an'),\n",
       " (2.8649770593271122, 'movies')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important words for American_movie_actors class\n",
    "sorted([(weights[index], word) for word, index in word_to_index.items()])[-1:-15:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-7.2215606516983524, 'of'),\n",
       " (-4.4236326599835056, 'politician'),\n",
       " (-3.7006406410501556, 'from'),\n",
       " (-2.787617534816401, 'to'),\n",
       " (-2.3817626414285775, 'democratic'),\n",
       " (-2.3511756609059202, 'states'),\n",
       " (-1.9544975672977596, 'he'),\n",
       " (-1.9125664117701597, 'united'),\n",
       " (-1.8476226873040478, 'governor'),\n",
       " (-1.7735524217387413, 'member'),\n",
       " (-1.7617972029084346, 'served'),\n",
       " (-1.6199405046473707, 'party'),\n",
       " (-1.5605207418086355, 'house'),\n",
       " (-1.3142585335306527, 'the'),\n",
       " (-1.2714361492602533, 'u')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important words for US_Democratic_Party_politicians class\n",
    "sorted([(weights[index], word) for word, index in word_to_index.items()])[:15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
