{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5_bT2akGkMG8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxLdPp6Jkdpr"
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w59qzQc6kQIf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\n",
      " * Starting OpenBSD Secure Shell server sshd\n",
      "start-stop-daemon: unable to set gid to 0 (Operation not permitted)\n",
      "   ...fail!\n",
      " * sshd is running\n",
      "Starting namenodes on [localhost]\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: namenode is running as process 163.  Stop it first and ensure /tmp/hadoop-jovyan-namenode.pid file is empty before retry.\n",
      "Starting datanodes\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: datanode is running as process 273.  Stop it first and ensure /tmp/hadoop-jovyan-datanode.pid file is empty before retry.\n",
      "Starting secondary namenodes [bdfdecdf0194]\n",
      "bdfdecdf0194: Warning: Permanently added 'bdfdecdf0194' (ED25519) to the list of known hosts.\n",
      "bdfdecdf0194: secondarynamenode is running as process 483.  Stop it first and ensure /tmp/hadoop-jovyan-secondarynamenode.pid file is empty before retry.\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 724.  Stop it first and ensure /tmp/hadoop-jovyan-resourcemanager.pid file is empty before retry.\n",
      "Starting nodemanagers\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: nodemanager is running as process 835.  Stop it first and ensure /tmp/hadoop-jovyan-nodemanager.pid file is empty before retry.\n",
      "WARNING: Use of this script to start the MR JobHistory daemon is deprecated.\n",
      "WARNING: Attempting to execute replacement \"mapred --daemon start\" instead.\n",
      "273 org.apache.hadoop.hdfs.server.datanode.DataNode\n",
      "163 org.apache.hadoop.hdfs.server.namenode.NameNode\n",
      "835 org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "483 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n",
      "724 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\n",
      "2246 sun.tools.jps.Jps -lm\n",
      "2219 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "Present Capacity: 1011236773888 (941.79 GB)\n",
      "DFS Remaining: 1011236749312 (941.79 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: bdfdecdf0194\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 14872047616 (13.85 GB)\n",
      "DFS Remaining: 1011236749312 (941.79 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 93.54%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Mar 30 22:37:26 UTC 2023\n",
      "Last Block Report: Thu Mar 30 22:35:08 UTC 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "chmod: changing permissions of '/home/jovyan/jupyter.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/nginx.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/error.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/access.log': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "! /home/jovyan/start-hadoop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yCF3WacTkgHi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-03-30 22:37:41,893 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# connect, context, session\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBlzZC-4khcR"
   },
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rlBZKkrHkisM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                 Size     Used  Available  Use%\n",
      "hdfs://localhost:9000  1006.9 G  291.6 M    939.9 G    0%\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3TAOdesAkk5u",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwx---   - root   supergroup          0 2023-03-30 22:38 /tmp\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-03-30 22:37 /user\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PYzwf4rKkl2h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ME29QNXZkm-G",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jovyan/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/.kaggle/kaggle.json\n",
    "{\"username\":\"vadrad\",\"key\":\"3f94f2ff67bffb0edd6ee239ecbd10d7\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M_aMy0frkoIn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q2D-ym9NkpT6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (1.24.3)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m20.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: kaggle==1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2.28.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (4.65.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle==1.5.3) (2022.12.7)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle==1.5.3) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle==1.5.3) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle==1.5.3) (3.4)\n",
      "Downloading page_views_sample.csv.zip to /home/jovyan/work\n",
      "100%|████████████████████████████████████████| 149M/149M [00:23<00:00, 7.72MB/s]\n",
      "100%|████████████████████████████████████████| 149M/149M [00:23<00:00, 6.56MB/s]\n",
      "Downloading documents_topics.csv.zip to /home/jovyan/work\n",
      "100%|████████████████████████████████████████| 121M/121M [00:19<00:00, 6.29MB/s]\n",
      "100%|████████████████████████████████████████| 121M/121M [00:19<00:00, 6.64MB/s]\n"
     ]
    }
   ],
   "source": [
    "! pip install -U urllib3 kaggle==1.5.3\n",
    "! kaggle competitions download -c outbrain-click-prediction -f page_views_sample.csv.zip\n",
    "! kaggle competitions download -c outbrain-click-prediction -f documents_topics.csv.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9ClGhf8Xkqzt",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  page_views_sample.csv.zip\n",
      "  inflating: page_views_sample.csv   \n",
      "\n",
      "Archive:  documents_topics.csv.zip\n",
      "  inflating: documents_topics.csv    \n",
      "\n",
      "2 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "! unzip '*.zip'\n",
    "! rm -rf *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aMlxuy2fkr36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -put page_views_sample.csv\n",
    "! hdfs dfs -put documents_topics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7EUhvMnk0lc"
   },
   "source": [
    "## Evaluation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7crPsvk12p"
   },
   "source": [
    "Data: outbrain click prediction\n",
    "\n",
    "Tasks:\n",
    "Using Spark RDD, DataFrame API and Python, calculate:\n",
    "\n",
    "**1**. Top 10 most visited document_ids in the page_views_sample log\n",
    "\n",
    "**2**. How many users have at least 2 different traffic_sources in the page_views_sample log (note the value is not a count, it's an encoded enum)\n",
    "\n",
    "**3***. Top 10 most visited topic_ids in page_views_sample log (use documents_topics table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocayt-wlk4Nq"
   },
   "source": [
    "The submission format is the result.json json file with top_10_documents, users and top_10_topics keys.\n",
    "For TOP-10 results, the answer must be written in the form of a sheet ordered from TOP-1 to TOP-10 with an id.\n",
    "\n",
    "result.json example:\n",
    "\n",
    "    {\n",
    "        \"top_10_documents\": [\n",
    "            111,\n",
    "            222,\n",
    "            333,\n",
    "            ...,\n",
    "            1010\n",
    "        ],\n",
    "        \"users\": 10000,\n",
    "        \"top_10_topics\": [\n",
    "            11,\n",
    "            22,\n",
    "            33,\n",
    "            ...,\n",
    "            101\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qc5yojolk5l8"
   },
   "source": [
    "### DataFrame API solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My2a4xW3k_QU"
   },
   "source": [
    "Let's try to use the DataFrame API first, since it's more high level - it should be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-JWlvIPilBnE",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "page_views_sample = se.read.csv(\"page_views_sample.csv\", header=True)\n",
    "documents_topics = se.read.csv(\"documents_topics.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xmy_eqdklFzB"
   },
   "source": [
    "### 1. Top 10 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoR8T8OElHIn",
    "tags": []
   },
   "source": [
    "To get 10 most popular documents we can just group the df by document id, count, sort them by count and show the head 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BM-0zqhhlISD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_10_documents = (\n",
    "    page_views_sample.groupBy(\"document_id\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .limit(10)\n",
    "    .rdd.map(lambda row: int(row.document_id))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZg5vh0jlNq0"
   },
   "source": [
    "### 2. Users with at least 2 traffic sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vez54ZLZlNEp"
   },
   "source": [
    "The sql function countDistinct can be one of the possible solutions. Here we just group views by user id, aggregate them by the number of distinct sources, filter it and count the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tgPL7pPolPfq",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "users_with_multiple_traffic_sources = (\n",
    "    page_views_sample.groupBy(\"uuid\")\n",
    "    .agg(countDistinct(\"traffic_source\").alias(\"distinct_traffic_sources\"))\n",
    "    .filter(\"distinct_traffic_sources >= 2\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brfef9HHlRCZ"
   },
   "source": [
    "### 3. Top 10 most visited topic ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fswoYHonlUP7"
   },
   "source": [
    "The idea here is the same as in the first task, the only thing we should do is to join the views table with the topics table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_9BmYshAlTo1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df = page_views_sample.join(documents_topics, on=\"document_id\", how=\"inner\")\n",
    "\n",
    "top_10_topics = (\n",
    "    joined_df.groupBy(\"topic_id\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .limit(10)\n",
    "    .rdd.map(lambda row: int(row.topic_id))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVQz_PXblWfR"
   },
   "source": [
    "#### Save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TIZZFh0JlYkS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = {\n",
    "    \"top_10_documents\": top_10_documents,\n",
    "    \"users\": users_with_multiple_traffic_sources,\n",
    "    \"top_10_topics\": top_10_topics,\n",
    "}\n",
    "\n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP8ilgZZlW4m"
   },
   "source": [
    "See the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0jJ8QzzclbQY",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_10_documents': [1811567,\n",
       "  234,\n",
       "  42744,\n",
       "  1858440,\n",
       "  1780813,\n",
       "  60164,\n",
       "  1790442,\n",
       "  1877626,\n",
       "  1821895,\n",
       "  732651],\n",
       " 'users': 98080,\n",
       " 'top_10_topics': [20, 16, 216, 136, 140, 143, 36, 97, 8, 269]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ct11GKXolceT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "!curl -F file=@result.json \"51.250.54.133:80/MDS-LSML1/vadrad/w4/1\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
