{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS configuration\n",
    "\n",
    "https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n",
      "<configuration>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.blocksize</name>\r\n",
      "\t\t<value>16m</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.replication</name>\r\n",
      "\t\t<value>1</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.permissions.enabled</name>\r\n",
      "\t\t<value>false</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.namenode.name.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/namenode</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.datanode.data.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/datanode</value>\r\n",
      "\t</property>\r\n",
      "</configuration>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datanode  namenode\r\n"
     ]
    }
   ],
   "source": [
    "! ls /usr/local/hadoop/hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum [-v] <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-concat <target path> <src path> <src path> ...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate] [-fs <path>]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy files to/from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Test file\" > test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 168504\r\n",
      "drwxr-sr-x 3 jovyan users     4096 Apr  9 08:19 cpu1\r\n",
      "-rw-rw-r-- 1 jovyan root     21898 Apr  7 16:24 hdfs-basics-seminar.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users      446 Apr  8 19:23 mapper2.py\r\n",
      "-rw-r--r-- 1 jovyan users      601 Apr  8 19:48 mapper3.py\r\n",
      "-rw-r--r-- 1 jovyan users      287 Apr  8 18:36 mapper.py\r\n",
      "-rw-r--r-- 1 jovyan users 57408674 Apr 11 19:43 mapreduce-hw-task-Copy1.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users 57408674 Apr 11 19:43 mapreduce-hw-task-Copy2.ipynb\r\n",
      "-rw-rw-r-- 1 jovyan root  57408674 Apr 10 13:01 mapreduce-hw-task.ipynb\r\n",
      "-rw-rw-r-- 1 jovyan root    244931 Apr  8 19:54 mapreduce-wordcount-seminar.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users      164 Apr  8 19:27 reducer2.py\r\n",
      "-rw-r--r-- 1 jovyan users      381 Apr  8 18:37 reducer.py\r\n",
      "-rw-r--r-- 1 jovyan users       57 Apr  9 10:51 result.json\r\n",
      "-rw-r--r-- 1 jovyan users        5 Apr  8 19:27 result.txt\r\n",
      "-rw-r--r-- 1 jovyan users       10 Apr  7 13:31 test2.txt\r\n",
      "-rw-r--r-- 1 jovyan users       10 Apr 12 21:01 test.txt\r\n",
      "drwxr-sr-x 1 jovyan root      4096 Apr  8 18:19 wiki\r\n",
      "drwxr-sr-x 1 jovyan root      4096 Mar 17 07:58 yandex_music\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/test.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal test.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-04-08 18:25 /test\r\n",
      "-rw-r--r--   1 jovyan supergroup         10 2023-04-03 18:53 /test.txt\r\n",
      "-rw-r--r--   1 jovyan supergroup          8 2023-04-07 13:31 /test3.txt\r\n",
      "drwxrwx---   - root   supergroup          0 2023-04-08 18:20 /tmp\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-04-09 09:24 /user\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-04-08 18:19 /wiki\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-04-08 18:23 /word-count\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-04-06 14:35 /yandex_music\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `test2.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyToLocal /test.txt test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file\r\n"
     ]
    }
   ],
   "source": [
    "! cat test2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from/to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /test.txt | wc -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/test3.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"1 2 3 4\" | hadoop fs -copyFromLocal - /test3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /test3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t<name>dfs.replication</name>\r\n",
      "\t\t<value>1</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 1 replication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -stat %r /test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 2 set: /test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -setrep 2 /test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -stat %r /test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t<name>dfs.blocksize</name>\r\n",
      "\t\t<value>16m</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 1 block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate.py\n"
     ]
    }
   ],
   "source": [
    "%%file generate.py\n",
    "import sys\n",
    "\n",
    "megabytes = int(sys.argv[1])\n",
    "string = \"{}M\\n\".format(megabytes)\n",
    "count = 1024 * 1024 * megabytes // len(string) - 1\n",
    "print(string * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./generate.py\", line 6, in <module>\r\n",
      "    print(string * count)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! python ./generate.py 1 | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/16.txt': No such file or directory\r\n",
      "rm: `/32.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm /16.txt /32.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 MB file\n",
    "! python ./generate.py 16 | hadoop fs -copyFromLocal - /16.txt\n",
    "\n",
    "# 32 MB file\n",
    "! python ./generate.py 32 | hadoop fs -copyFromLocal - /32.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://localhost:9870/fsck?ugi=jovyan&files=1&blocks=1&path=%2F\r\n",
      "FSCK started by jovyan (auth:SIMPLE) from /127.0.0.1 for path / at Tue Oct 27 18:01:09 GMT 2020\r\n",
      "\r\n",
      "/ <dir>\r\n",
      "/16.txt 16777213 bytes, replicated: replication=1, 1 block(s):  OK\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741827_1003 len=16777213 Live_repl=1\r\n",
      "\r\n",
      "/32.txt 33554429 bytes, replicated: replication=1, 2 block(s):  OK\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741828_1004 len=16777216 Live_repl=1\r\n",
      "1. BP-417124827-172.18.0.2-1603454947911:blk_1073741829_1005 len=16777213 Live_repl=1\r\n",
      "\r\n",
      "/test.txt 10 bytes, replicated: replication=2, 1 block(s):  Under replicated BP-417124827-172.18.0.2-1603454947911:blk_1073741825_1001. Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741825_1001 len=10 Live_repl=1\r\n",
      "\r\n",
      "/test3.txt 8 bytes, replicated: replication=1, 1 block(s):  OK\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741826_1002 len=8 Live_repl=1\r\n",
      "\r\n",
      "/tmp <dir>\r\n",
      "/tmp/hadoop-yarn <dir>\r\n",
      "/tmp/hadoop-yarn/staging <dir>\r\n",
      "/tmp/hadoop-yarn/staging/history <dir>\r\n",
      "/tmp/hadoop-yarn/staging/history/done <dir>\r\n",
      "/tmp/hadoop-yarn/staging/history/done_intermediate <dir>\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t1\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t7\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t50331660 B\r\n",
      " Total files:\t4\r\n",
      " Total blocks (validated):\t5 (avg. block size 10066332 B)\r\n",
      " Minimally replicated blocks:\t5 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t1 (20.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t1\r\n",
      " Average block replication:\t1.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t1 (16.666666 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Tue Oct 27 18:01:09 GMT 2020 in 4 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs fsck / -files -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What HDFS looks like on a disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t<name>dfs.namenode.name.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/namenode</value>\r\n",
      "--\r\n",
      "\t\t<name>dfs.datanode.data.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/datanode</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 1 dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/hdfs/datanode:\r\n",
      "current  in_use.lock\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current:\r\n",
      "BP-417124827-172.18.0.2-1603454947911  VERSION\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911:\r\n",
      "current  scanner.cursor  tmp\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current:\r\n",
      "finalized  rbw\tVERSION\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized:\r\n",
      "subdir0\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0:\r\n",
      "subdir0\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0:\r\n",
      "blk_1073741825\t\t  blk_1073741827\t    blk_1073741829\r\n",
      "blk_1073741825_1001.meta  blk_1073741827_1003.meta  blk_1073741829_1005.meta\r\n",
      "blk_1073741826\t\t  blk_1073741828\r\n",
      "blk_1073741826_1002.meta  blk_1073741828_1004.meta\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/rbw:\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/tmp:\r\n"
     ]
    }
   ],
   "source": [
    "! ls -R $HADOOP_HOME/hdfs/datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741827 <==\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741826_1002.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000,��\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741829 <==\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741829_1005.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000�FmG�FmG�FmG�FmG�FmG�FmG�\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741828 <==\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741825_1001.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000\u000f[H�\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741828_1004.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000�FmG�FmG�FmG�FmG�FmG�FmG�\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741826 <==\r\n",
      "1 2 3 4\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741825 <==\r\n",
      "Test file\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741827_1003.meta <==\r\n",
      "Ԩ!"
     ]
    }
   ],
   "source": [
    "! find $HADOOP_HOME/hdfs/datanode | grep blk | xargs head -c 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/hdfs/namenode:\r\n",
      "current  in_use.lock\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/namenode/current:\r\n",
      "edits_0000000000000000001-0000000000000000009  fsimage_0000000000000000000.md5\r\n",
      "edits_inprogress_0000000000000000010\t       seen_txid\r\n",
      "fsimage_0000000000000000000\t\t       VERSION\r\n"
     ]
    }
   ],
   "source": [
    "! ls -R $HADOOP_HOME/hdfs/namenode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
